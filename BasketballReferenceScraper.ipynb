{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a Mac, need to specify path that contains chromedriver\n",
    "driver_path = \"/Users/Matthew/Documents/Consulting/Lectures/Webscraping/chromedriver\"\n",
    "year = '1973'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(box,tables,table):\n",
    "    count = 1\n",
    "    header = tables[-3].findAll(\"tr\")[1].text.split('\\n')\n",
    "    header.pop(0)\n",
    "    header.pop(-1)\n",
    "    for row in table.findAll(\"tr\"):\n",
    "        length = len(table.findAll(\"tr\"))\n",
    "        if count == length:\n",
    "            try:\n",
    "                name = row.find_all('a')[0].text = 'Total'\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                name = row.find_all('a')[0].text\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            cells = row.findAll(\"td\")\n",
    "            cells = [ele.text for ele in cells]\n",
    "            if len(cells)>1:\n",
    "                cells.insert(0, name)\n",
    "                box.append(cells)\n",
    "        except:\n",
    "            pass\n",
    "        count += 1\n",
    "    return header\n",
    "        \n",
    "def create_minutes(box):\n",
    "    box['Minutes']=box['MP'].str.split(\":\", expand=True).fillna(0)[0].apply(pd.to_numeric)\n",
    "    box['Seconds']=box['MP'].str.split(\":\", expand=True).fillna(0)[1].apply(pd.to_numeric)\n",
    "    box['_MP']=box['Minutes']+(box['Seconds']/60)\n",
    "    box['_MP']=box.loc[:,'_MP':].div(box.iloc[-1]['_MP':]/5)\n",
    "    box=box[['Player','_MP']]\n",
    "    box=box.iloc[:-1]\n",
    "    box = box.T.reset_index()\n",
    "    box.columns = box.iloc[0]\n",
    "    box = box[1:]\n",
    "    box=box.drop('Player',axis=1)\n",
    "    box.fillna(0,inplace=True)\n",
    "    return box\n",
    "\n",
    "def get_roster(roster_url):\n",
    "    ##Connect to a website using a proxy server\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    #chrome_options.add_argument('--proxy-server=%s' % PROXY)\n",
    "    chrome_options.add_argument('--headless')\n",
    "\n",
    "    # If not using Mac\n",
    "    browser = webdriver.Chrome(executable_path = driver_path, chrome_options=chrome_options)\n",
    "    browser.get(roster_url)\n",
    "\n",
    "    ##Grab the raw webpage\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML,\"html.parser\")\n",
    "    tables = soup.findAll(\"table\")\n",
    "\n",
    "    players = []\n",
    "    for row in tables[0].findAll(\"tr\"):\n",
    "        try:\n",
    "            player = row.find('a').text\n",
    "            players.append(player)\n",
    "        except:\n",
    "            pass\n",
    "    return players\n",
    "        \n",
    "def get_box_scores(games,homes,differentials):\n",
    "    boxs = []\n",
    "    game_count = 1\n",
    "    for game,home,diff in zip(games,homes,differentials):\n",
    "        browser.get(game)\n",
    "        innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "        soup = BeautifulSoup(innerHTML,\"html.parser\")\n",
    "        tables = soup.findAll(\"table\")\n",
    "\n",
    "        box = []\n",
    "        box2 = []\n",
    "        table_count = 1\n",
    "        try:\n",
    "            team_check = browser.find_element_by_id('content').text.split('\\n')[0].split('at')[0].strip()\n",
    "        except:\n",
    "            team_check = browser.find_element_by_id('content').text.split('\\n')[0].split('vs')[0].strip()\n",
    "        for table in tables[-3:]:\n",
    "            if table_count == 1:\n",
    "                if team_check==team:\n",
    "                    header=add_data(box,tables,table)\n",
    "                else:\n",
    "                    header=add_data(box2,tables,table)\n",
    "            else:\n",
    "                if team_check==team:\n",
    "                    header=add_data(box2,tables,table)\n",
    "                else: \n",
    "                    header=add_data(box,tables,table)\n",
    "            table_count += 1\n",
    "        try:\n",
    "            box = pd.DataFrame(box, columns = header)\n",
    "            box2 = pd.DataFrame(box2, columns = header)\n",
    "            box=create_minutes(box)\n",
    "            box2=create_minutes(box2)\n",
    "            box['Differential'] = diff\n",
    "            box['Location']=home\n",
    "            box = pd.concat([box, box2], axis=1)\n",
    "            if len(boxs)==0:\n",
    "                boxs=box.copy()\n",
    "            else:\n",
    "                boxs=pd.concat([boxs, box], axis=0)\n",
    "            print(game_count,home,diff)\n",
    "        except Exception as e:\n",
    "            print('cannot pull game')\n",
    "        game_count+=1\n",
    "        time.sleep(1)\n",
    "    return boxs\n",
    "\n",
    "def get_games(url):\n",
    "    browser.get(url)\n",
    "    ##Grab the raw webpage\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML,\"html.parser\")\n",
    "    tables = soup.findAll(\"table\")\n",
    "\n",
    "    games = []\n",
    "    homes = []\n",
    "    differentials = []\n",
    "    try:\n",
    "        tbl = tables[-2].findAll(\"tr\")\n",
    "    except:\n",
    "        tbl = tables[0].findAll(\"tr\")\n",
    "    for row in tbl:\n",
    "        td = row.findAll(\"td\")\n",
    "        try:\n",
    "            game = \"https://www.basketball-reference.com\"+td[2].find_all('a')[0].get('href')\n",
    "            diff=float(td[7].text)-float(td[8].text)\n",
    "            differentials.append(diff)\n",
    "            if td[3].text:\n",
    "                homes.append(td[3].text)\n",
    "            else:\n",
    "                homes.append('H')\n",
    "            games.append(game)\n",
    "        except:\n",
    "            pass\n",
    "    print(len(games))\n",
    "    return games,homes,differentials\n",
    "\n",
    "def rename_roster(box,roster):\n",
    "    global roster_t\n",
    "    try:\n",
    "        box[roster]\n",
    "        roster2=roster.copy()\n",
    "    except Exception as e:\n",
    "        error = str(e).split(' not ')[0].split('\"')[1]\n",
    "        print(error)\n",
    "        roster2 = [x for x in roster if x not in str(e).split(' not ')[0]]\n",
    "    roster_t = []\n",
    "    for r in roster2:\n",
    "        roster_t.append(r + \"_T\")\n",
    "    col_rename_dict = {i:j for i,j in zip(roster2,roster_t)}\n",
    "    print(col_rename_dict)\n",
    "    box2 = box.rename(columns=col_rename_dict)\n",
    "    return box2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all teams in a list and their links\n",
    "url1 = 'https://www.basketball-reference.com/leagues/NBA_'+year+'.html'\n",
    "\n",
    "##Connect to a website using a proxy server\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "#chrome_options.add_argument('--proxy-server=%s' % PROXY)\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "# If not using Mac\n",
    "browser = webdriver.Chrome(executable_path = driver_path, chrome_options=chrome_options)\n",
    "browser.get(url1)\n",
    "\n",
    "##Grab the raw webpage\n",
    "innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "soup = BeautifulSoup(innerHTML,\"html.parser\")\n",
    "tables = soup.findAll(\"table\")\n",
    "\n",
    "teams =[]\n",
    "teamz = []\n",
    "for row in tables[0].findAll(\"tr\"):\n",
    "    try:\n",
    "        a = row.find_all('a', href=True)\n",
    "        for b in a:\n",
    "            teamz.append(b['href'])\n",
    "            teams.append(b.text)\n",
    "    except:\n",
    "        pass\n",
    "teamz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Box Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Connect to a website using a proxy server\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "#chrome_options.add_argument('--proxy-server=%s' % PROXY)\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "boxs = pd.DataFrame()\n",
    "# If not using Mac\n",
    "browser = webdriver.Chrome(executable_path = driver_path, chrome_options=chrome_options)\n",
    "\n",
    "for team,name in zip(teams,teamz):\n",
    "    try:\n",
    "        print(team)\n",
    "        url = 'https://www.basketball-reference.com'+name.split('.html')[0]+'_games.html'\n",
    "        roster_url = 'https://www.basketball-reference.com'+name\n",
    "\n",
    "        roster = get_roster(roster_url)\n",
    "        games,homes,differentials = get_games(url)\n",
    "        team_box = get_box_scores(games,homes,differentials)\n",
    "        team_box2 = rename_roster(team_box,roster)\n",
    "        if len(boxs)==0:\n",
    "            boxs=team_box2.copy()\n",
    "        else:\n",
    "            boxs=pd.concat([boxs, team_box2], axis=0)\n",
    "        print(len(boxs))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxs.to_excel('BoxScores/'+str(int(year))+'box.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
